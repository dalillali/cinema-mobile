{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalillali/cinema-mobile/blob/main/(2)_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recap"
      ],
      "metadata": {
        "id": "w7uv9KU_b4PG"
      },
      "id": "w7uv9KU_b4PG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numpy"
      ],
      "metadata": {
        "id": "CxS5zx9RfEbS"
      },
      "id": "CxS5zx9RfEbS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy (Numerical Python) est une bibliothèque fondamentale pour le calcul numérique en Python, souvent utilisée en machine learning (ML) et deep learning (DL). Elle offre des structures de données puissantes (notamment des tableaux multidimensionnels, ou \"arrays\") et un ensemble de fonctions mathématiques efficaces pour effectuer des calculs sur de grands ensembles de données.\n",
        "\n",
        "- Temps: Optimisé pour des opérations vectorielles\n",
        "- Mémoire: écrit en C et utilise une gestion mémoire efficace. (stockage contigu, même type, évite les copies inutiles, ...)\n",
        "\n",
        "- fournit des fonctions pour les statistiques, l'algèbre linéaire, ...\n",
        "\n",
        "Pour plus de détails: [EMSIMa/ML-DL/Numpy](https://github.com/EMSIMa/ML-DL/blob/main/00a_Numpy.ipynb)"
      ],
      "metadata": {
        "id": "R6tlAoB_fBIW"
      },
      "id": "R6tlAoB_fBIW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3478591-7849-4330-ac7e-12f3cb880335",
      "metadata": {
        "id": "e3478591-7849-4330-ac7e-12f3cb880335"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "np.set_printoptions(precision=4, suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c6757b-017b-4090-9800-8f8aecd0f558",
      "metadata": {
        "id": "13c6757b-017b-4090-9800-8f8aecd0f558"
      },
      "outputs": [],
      "source": [
        "my_arr = np.arange(1_000_000)\n",
        "my_list = list(range(1_000_000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9df3ee07-f469-434a-ae42-f1e4fd19fe6e",
      "metadata": {
        "id": "9df3ee07-f469-434a-ae42-f1e4fd19fe6e"
      },
      "outputs": [],
      "source": [
        "%timeit my_arr2 = my_arr * 2\n",
        "%timeit my_list2 = [x * 2 for x in my_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un ndarray est un conteneur multidimensionnel générique pour des données homogènes, c’est-à-dire que tous les éléments doivent être du même type."
      ],
      "metadata": {
        "id": "JqHlLT0PjuBM"
      },
      "id": "JqHlLT0PjuBM"
    },
    {
      "cell_type": "code",
      "source": [
        "l = [\"hi\", 987, True]"
      ],
      "metadata": {
        "id": "R7NLwiolj1Ks"
      },
      "id": "R7NLwiolj1Ks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar = np.array(l)"
      ],
      "metadata": {
        "id": "ISomy0X5j9qa"
      },
      "id": "ISomy0X5j9qa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ar"
      ],
      "metadata": {
        "id": "eiCWNDotkXnn"
      },
      "id": "eiCWNDotkXnn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in l:\n",
        "  print(type(i))"
      ],
      "metadata": {
        "id": "SyjH137Yk1m7"
      },
      "id": "SyjH137Yk1m7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, _ in enumerate(ar):\n",
        "  print(type(ar[i]))"
      ],
      "metadata": {
        "id": "vrpiv_Avkoc0"
      },
      "id": "vrpiv_Avkoc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6eb2a17-d98e-4156-9328-8140d7bf8580",
      "metadata": {
        "id": "b6eb2a17-d98e-4156-9328-8140d7bf8580"
      },
      "outputs": [],
      "source": [
        "data = np.random.randn(5, 5)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea629b5-1989-407f-a71b-e6220b14907b",
      "metadata": {
        "id": "7ea629b5-1989-407f-a71b-e6220b14907b"
      },
      "outputs": [],
      "source": [
        "data * 10 - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "584b6bff-3aaa-405b-a997-09cdc71d7acd",
      "metadata": {
        "id": "584b6bff-3aaa-405b-a997-09cdc71d7acd"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2119dd22-0471-478a-8538-c346a57c398d",
      "metadata": {
        "id": "2119dd22-0471-478a-8538-c346a57c398d"
      },
      "outputs": [],
      "source": [
        "data.dtype"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0, 2] = True # ou data[0][2]"
      ],
      "metadata": {
        "id": "MBLDaIPplRYD"
      },
      "id": "MBLDaIPplRYD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "3JY8lyPFloQA"
      },
      "id": "3JY8lyPFloQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b8f5368f-c8c0-4d32-a732-3cd40335a427",
      "metadata": {
        "id": "b8f5368f-c8c0-4d32-a732-3cd40335a427"
      },
      "source": [
        "Indexation et tranchage élémentaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71880e3b-5557-4daa-b4d7-db6e56918671",
      "metadata": {
        "id": "71880e3b-5557-4daa-b4d7-db6e56918671"
      },
      "outputs": [],
      "source": [
        "arr = np.arange(15)\n",
        "arr[5:8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce92a676-da20-47df-9336-afdcd9b426ec",
      "metadata": {
        "id": "ce92a676-da20-47df-9336-afdcd9b426ec"
      },
      "outputs": [],
      "source": [
        "arr[5:8] = 42\n",
        "arr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e677e1e0-413f-41ea-b0d0-984c0aed80af",
      "metadata": {
        "id": "e677e1e0-413f-41ea-b0d0-984c0aed80af"
      },
      "source": [
        "### Indexation booléenne\n",
        "Prenons un exemple dans lequel nous avons quelques données dans un tableau, ainsi qu’un tableau de noms avec des doublons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435b58ac-b487-407d-a526-d167eee33766",
      "metadata": {
        "id": "435b58ac-b487-407d-a526-d167eee33766"
      },
      "outputs": [],
      "source": [
        "villes = np.array(['Marrakech', 'Fes', 'Ifran', 'Marrakech', 'Casa', 'Rabat', 'Rabat', 'Tanger'])\n",
        "data = np.abs(np.random.randn(8, 5))*1000\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42215637-43ae-41af-828d-188564a0d5ab",
      "metadata": {
        "id": "42215637-43ae-41af-828d-188564a0d5ab"
      },
      "outputs": [],
      "source": [
        "villes == 'Marrakech'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88b35c3-b311-4f16-826c-461dd102edeb",
      "metadata": {
        "id": "e88b35c3-b311-4f16-826c-461dd102edeb"
      },
      "outputs": [],
      "source": [
        "data[villes == 'Marrakech']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607749d4-512f-44af-89b4-1a2387e50256",
      "metadata": {
        "id": "607749d4-512f-44af-89b4-1a2387e50256"
      },
      "outputs": [],
      "source": [
        "data[villes == 'Marrakech', 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01182f03-f26e-43b6-abed-d3e80c3f21d0",
      "metadata": {
        "id": "01182f03-f26e-43b6-abed-d3e80c3f21d0"
      },
      "outputs": [],
      "source": [
        "data[data < 0] = 0\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stats"
      ],
      "metadata": {
        "id": "PCTqJJP_soqc"
      },
      "id": "PCTqJJP_soqc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ba5736-573f-438d-b260-52d31d992606",
      "metadata": {
        "id": "24ba5736-573f-438d-b260-52d31d992606"
      },
      "outputs": [],
      "source": [
        "arr = np.random.randn(3, 6)\n",
        "arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f408ad02-6010-4b53-aef0-6ed5efccdfb7",
      "metadata": {
        "id": "f408ad02-6010-4b53-aef0-6ed5efccdfb7"
      },
      "outputs": [],
      "source": [
        "arr.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22ea6d19-c8c6-4bbd-adce-8263023ec0c0",
      "metadata": {
        "id": "22ea6d19-c8c6-4bbd-adce-8263023ec0c0"
      },
      "outputs": [],
      "source": [
        "arr.sum()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_sum = arr.sum(axis=0)\n",
        "print(column_sum)\n",
        "\n",
        "row_sum = arr.sum(axis=1)\n",
        "print(row_sum)"
      ],
      "metadata": {
        "id": "-rlcrT1vtLuR"
      },
      "id": "-rlcrT1vtLuR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas"
      ],
      "metadata": {
        "id": "UT2d1o1Yt8ds"
      },
      "id": "UT2d1o1Yt8ds"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas est une bibliothèque Python essentielle pour la manipulation et l’analyse de données.\n",
        "Elle est construite au-dessus de NumPy et propose des structures de données de haut niveau qui facilitent la manipulation, l’analyse et la visualisation de données.\n",
        "\n",
        "\\* Pour plus de détails: [EMSIMa/ML-DL/Pandas](https://github.com/EMSIMa/ML-DL/blob/main/00b_Pandas.ipynb)\n"
      ],
      "metadata": {
        "id": "eHpNavB1nLfq"
      },
      "id": "eHpNavB1nLfq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pourquoi utiliser Pandas en machine learning ?\n",
        "- Manipulation de données : Pandas permet de manipuler des données avec des opérations simples et intuitives, comme le filtrage, le tri, l'agrégation, et bien plus.\n",
        "- Nettoyage des données : Elle offre des outils puissants pour gérer les valeurs manquantes, corriger les types de données, et réaliser des transformations.\n",
        "- Interopérabilité : Compatible avec NumPy, les DataFrames peuvent être facilement utilisés dans des pipelines de machine learning avec d'autres bibliothèques comme Scikit-Learn."
      ],
      "metadata": {
        "id": "c0K2TeC8nSyc"
      },
      "id": "c0K2TeC8nSyc"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Ville': ['Marrakech', 'Fes', 'Ifrane', 'Casa', 'Rabat'],\n",
        "    'Population': [930000, 1150000, 130000, 3500000, 1200000],\n",
        "    'Température': [999.0, 20.5, 18.3, 25.5, 22.1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "h7t8DEi6t8BG"
      },
      "id": "h7t8DEi6t8BG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().astype(int)"
      ],
      "metadata": {
        "id": "P_H_UGSut5WK"
      },
      "id": "P_H_UGSut5WK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('fichier.csv', index=False)"
      ],
      "metadata": {
        "id": "TJS8xk3uoNGt"
      },
      "id": "TJS8xk3uoNGt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_2 = pd.read_csv('fichier.csv')\n",
        "df_2.head(1)"
      ],
      "metadata": {
        "id": "BTPFj-idpa_j"
      },
      "id": "BTPFj-idpa_j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Population'].head(3)"
      ],
      "metadata": {
        "id": "OXpRdgNgpdxg"
      },
      "id": "OXpRdgNgpdxg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Température'] > 40]"
      ],
      "metadata": {
        "id": "Kvhr-Bunp1sl"
      },
      "id": "Kvhr-Bunp1sl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.at[0, 'Population'] = np.nan"
      ],
      "metadata": {
        "id": "H88hM_a8qiJr"
      },
      "id": "H88hM_a8qiJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull()"
      ],
      "metadata": {
        "id": "3_uDctgJp2Tn"
      },
      "id": "3_uDctgJp2Tn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Population'] = df['Population'].fillna(df['Population'].mean()).astype(int)"
      ],
      "metadata": {
        "id": "NSCS6PTuqCLQ"
      },
      "id": "NSCS6PTuqCLQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "S8RQeug0rB8A"
      },
      "id": "S8RQeug0rB8A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.at[1, 'Population'] = 1200001"
      ],
      "metadata": {
        "id": "_PuTdQAarDqO"
      },
      "id": "_PuTdQAarDqO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iat[2, 2] = 19.42"
      ],
      "metadata": {
        "id": "quMHM9AHsAUm"
      },
      "id": "quMHM9AHsAUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0:3, 1:3]"
      ],
      "metadata": {
        "id": "mcIS_DQTxeDF"
      },
      "id": "mcIS_DQTxeDF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Population'] = df['Population'].replace(1200000, 1)"
      ],
      "metadata": {
        "id": "IyKF-7HjsMeK"
      },
      "id": "IyKF-7HjsMeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Population'] > 1000000, 'Population'] = 1_000_001"
      ],
      "metadata": {
        "id": "Og-rX03LsctI"
      },
      "id": "Og-rX03LsctI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Ville'] == 'Marrakech', 'Ville'] = 'Marrakesh'"
      ],
      "metadata": {
        "id": "paa6T01uu3q1"
      },
      "id": "paa6T01uu3q1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df['Ville'] == 'Fes', ['Population', 'Ville']] = [1400000, 'Fès']"
      ],
      "metadata": {
        "id": "vef5ul1pvQ3K"
      },
      "id": "vef5ul1pvQ3K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['rand'] = np.random.randint(0, 100, size=len(df))"
      ],
      "metadata": {
        "id": "s0DEWZC1wCfg"
      },
      "id": "s0DEWZC1wCfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "qKqwp1nkwffP"
      },
      "id": "qKqwp1nkwffP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation des Données : Data Cleaning et Feature Engineering"
      ],
      "metadata": {
        "id": "a6DcgxeQbru7"
      },
      "id": "a6DcgxeQbru7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La qualité des données est la pierre angulaire de tout projet de Machine Learning ou Deep Learning. Avant de plonger dans la modélisation, il est essentiel de s'assurer que vos données sont propres, cohérentes et enrichies de caractéristiques pertinentes. Dans cette section, nous allons explorer  les techniques de data cleaning (nettoyage des données) et de feature engineering (ingénierie des caractéristiques).\n",
        "\n",
        "Le data cleaning et le feature engineering sont des étapes cruciales qui permettent :\n",
        "\n",
        "- Une meilleure interprétabilité,\n",
        "- Réduire la complexité du modèle,\n",
        "- Améliorer la performance du modèle,\n",
        "- Accélérer le temps d'entraînement.\n",
        "\n"
      ],
      "metadata": {
        "id": "oMa5e6CscG-2"
      },
      "id": "oMa5e6CscG-2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Données Tabulaires"
      ],
      "metadata": {
        "id": "sintYkuKcw4c"
      },
      "id": "sintYkuKcw4c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les données tabulaires sont omniprésentes : bases de données, fichiers CSV, feuilles de calcul. Elles sont structurées en lignes et colonnes, chaque ligne représentant une observation et chaque colonne une variable."
      ],
      "metadata": {
        "id": "l75X7gCzc2b4"
      },
      "id": "l75X7gCzc2b4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'un DataFrame synthétique\n",
        "\n",
        "data = {\n",
        "    'age': np.random.randint(18, 80, size=100),\n",
        "    'revenu': np.random.normal(50000, 15000, size=100),\n",
        "    'genre': np.random.choice(['Homme', 'Femme'], size=100),\n",
        "    'statut_marital': np.random.choice(['Célibataire', 'Marié', 'Divorcé'], size=100),\n",
        "    'nombre_enfants': np.random.randint(0, 5, size=100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduisons des valeurs manquantes et des doublons\n",
        "df.loc[np.random.choice(df.index, size=10, replace=False), 'revenu'] = np.nan\n",
        "df.loc[np.random.choice(df.index, size=5, replace=False), 'genre'] = None\n",
        "\n",
        "# Utiliser pd.concat pour ajouter des doublons\n",
        "df = pd.concat([df, df.iloc[0:3]], ignore_index=True)  # Ajout de doublons\n",
        "\n",
        "# Affichage des premières lignes du DataFrame\n",
        "df.sample(15)"
      ],
      "metadata": {
        "id": "onHRINV7wcda"
      },
      "id": "onHRINV7wcda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "LuE3JAgvgKHg"
      },
      "id": "LuE3JAgvgKHg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traitement des Valeurs Manquantes\n",
        "\n",
        "1. Suppression des Lignes avec des Valeurs Manquantes:"
      ],
      "metadata": {
        "id": "NvJNdFIghfiR"
      },
      "id": "NvJNdFIghfiR"
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df.dropna()\n",
        "print(f\"Nombre de lignes après suppression des valeurs manquantes : {len(df_cleaned)}\")"
      ],
      "metadata": {
        "id": "Xh1Dq6_JgKVU"
      },
      "id": "Xh1Dq6_JgKVU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputation de la colonne 'revenu' avec la moyenne\n",
        "df['revenu'] = df['revenu'].fillna(df['revenu'].mean())\n",
        "\n",
        "# Imputation de la colonne 'genre' avec la valeur la plus fréquente\n",
        "df['genre'] = df['genre'].fillna(df['genre'].mode()[0])"
      ],
      "metadata": {
        "id": "dZCYbk5UhpLt"
      },
      "id": "dZCYbk5UhpLt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Gestion des Doublons:"
      ],
      "metadata": {
        "id": "AUvNCBZ_jrbz"
      },
      "id": "AUvNCBZ_jrbz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Nombre de doublons\n",
        "print(f\"Nombre de doublons avant suppression : {df.duplicated().sum()}\")\n",
        "\n",
        "# Suppression des doublons\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "print(f\"Nombre de doublons après suppression : {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "NeOe_ZIajW2G"
      },
      "id": "NeOe_ZIajW2G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Détection et Traitement des Valeurs Aberrantes"
      ],
      "metadata": {
        "id": "Y3pCvN72j-HG"
      },
      "id": "Y3pCvN72j-HG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajouter des valeurs aberrantes dans 'revenu'\n",
        "df.loc[0, 'revenu'] = 300000  # Très grande valeur aberrante\n",
        "df.loc[1, 'revenu'] = -50000   # Valeur négative aberrante"
      ],
      "metadata": {
        "id": "A0xY8C2tlmRB"
      },
      "id": "A0xY8C2tlmRB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajouter une colonne de dates avec des formats différents\n",
        "df['date_inscription'] = np.random.choice(['2021/01/05', '05-02-2021', 'March 3, 2021'], size=len(df))"
      ],
      "metadata": {
        "id": "YIYCg4Igltn2"
      },
      "id": "YIYCg4Igltn2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Histogramme du revenu\n",
        "sns.histplot(df['revenu'], kde=True)\n",
        "plt.title('Distribution du Revenu')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MbY8CDa5jx9T"
      },
      "id": "MbY8CDa5jx9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajout d'une colonne de dates avec des formats différents\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "6eDpfaBWkARG"
      },
      "id": "6eDpfaBWkARG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = df['revenu'].mean()\n",
        "std_dev = df['revenu'].std()"
      ],
      "metadata": {
        "id": "y-7TAoA3krgJ"
      },
      "id": "y-7TAoA3krgJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de la moyenne et de l'écart-type\n",
        "mean = df['revenu'].mean()\n",
        "std_dev = df['revenu'].std()\n",
        "\n",
        "# Détecter les valeurs aberrantes (à plus de 3 écarts-types de la moyenne)\n",
        "seuil_bas = mean - 3 * std_dev\n",
        "seuil_haut = mean + 3 * std_dev\n",
        "\n",
        "# Valeurs aberrantes détectées avec l'écart-type\n",
        "valeurs_aberrantes = df[(df['revenu'] < seuil_bas) | (df['revenu'] > seuil_haut)]\n",
        "valeurs_aberrantes"
      ],
      "metadata": {
        "id": "Bk6OodkSk1u7"
      },
      "id": "Bk6OodkSk1u7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[(df['revenu'] >= seuil_bas) & (df['revenu'] <= seuil_haut)]"
      ],
      "metadata": {
        "id": "KKr2lAxuk6P7"
      },
      "id": "KKr2lAxuk6P7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "formats = [\"%Y/%m/%d\", \"%d-%m-%Y\", \"%B %d, %Y\", \"%Y.%m.%d\", \"%m-%d-%Y\"]\n",
        "\n",
        "# Fonction pour essayer plusieurs formats de dates\n",
        "def parse_date(date_str):\n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "    # Si aucun format ne fonctionne, retourne NaT\n",
        "    return pd.NaT\n",
        "\n",
        "# Appliquer la fonction de conversion à la colonne 'date_inscription'\n",
        "df['date_inscription'] = df['date_inscription'].apply(parse_date)\n",
        "\n",
        "# Convertir toutes les dates en un format unifié (par exemple, \"YYYY-MM-DD\")\n",
        "df['date_inscription'] = df['date_inscription'].dt.strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "-2S7yAcipBk4"
      },
      "id": "-2S7yAcipBk4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(3)"
      ],
      "metadata": {
        "id": "IGDnCM5grEkm"
      },
      "id": "IGDnCM5grEkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Engineering pour les Données Tabulaires"
      ],
      "metadata": {
        "id": "xlNruck0sIXX"
      },
      "id": "xlNruck0sIXX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. One-Hot Encoding"
      ],
      "metadata": {
        "id": "dvH-f8VZs6Eh"
      },
      "id": "dvH-f8VZs6Eh"
    },
    {
      "cell_type": "code",
      "source": [
        "# Encodage des variables catégoriques\n",
        "df_encoded = pd.get_dummies(df, columns=['genre', 'statut_marital'])\n",
        "\n",
        "# Affichage des premières lignes\n",
        "df_encoded.head()"
      ],
      "metadata": {
        "id": "mu7X29XMrGbH"
      },
      "id": "mu7X29XMrGbH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Normalisation et Standardisation"
      ],
      "metadata": {
        "id": "QHyU37_HtIFP"
      },
      "id": "QHyU37_HtIFP"
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['revenu'], kde=True)\n",
        "plt.title('Distribution du Revenu')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D1nc2WxrtTvp"
      },
      "id": "D1nc2WxrtTvp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Sélection des variables numériques\n",
        "numeric_features = ['age', 'revenu', 'nombre_enfants']\n",
        "\n",
        "# Initialisation du scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Application de la normalisation\n",
        "df_encoded[numeric_features] = scaler.fit_transform(df_encoded[numeric_features])"
      ],
      "metadata": {
        "id": "9FzZIH0vsW6a"
      },
      "id": "9FzZIH0vsW6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df_encoded['revenu'], kde=True)\n",
        "plt.title('Distribution du Revenu après MinMaxScaler')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yhGkrPKUtrE1"
      },
      "id": "yhGkrPKUtrE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Création de Nouvelles Caractéristiques"
      ],
      "metadata": {
        "id": "FpKqtFE6uT7p"
      },
      "id": "FpKqtFE6uT7p"
    },
    {
      "cell_type": "code",
      "source": [
        "# Éviter la division par zéro\n",
        "df_encoded['nombre_enfants'] = df_encoded['nombre_enfants'].replace(0, 1)\n",
        "\n",
        "# Création de la nouvelle caractéristique\n",
        "df_encoded['revenu_par_enfant'] = df_encoded['revenu'] / df_encoded['nombre_enfants']"
      ],
      "metadata": {
        "id": "p53sp3hsuTme"
      },
      "id": "p53sp3hsuTme",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "PnZv7gdGyLu5"
      },
      "id": "PnZv7gdGyLu5"
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d camnugent/california-housing-prices -q\n",
        "!unzip california-housing-prices.zip"
      ],
      "metadata": {
        "id": "OUTWL1U7vKnl"
      },
      "id": "OUTWL1U7vKnl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('housing.csv')\n",
        "df = df.dropna()\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "HgVegYNEUFkx"
      },
      "id": "HgVegYNEUFkx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1, random_state=2)\n",
        "train_df = df[:17000]\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = df[17000:]\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_df"
      ],
      "metadata": {
        "id": "qrpYlSQlW6st"
      },
      "id": "qrpYlSQlW6st",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En observant la corrélation entre les caractéristiques total_rooms, total_bedrooms et households, nous pouvons identifier des relations fortes. La corrélation entre deux variables indique dans quelle mesure elles varient ensemble. Si la corrélation est élevée (proche de 1 ou -1), cela signifie que ces variables contiennent une information redondante."
      ],
      "metadata": {
        "id": "0qjS6WJSWvq2"
      },
      "id": "0qjS6WJSWvq2"
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[['total_rooms', 'total_bedrooms', 'households']].corr()"
      ],
      "metadata": {
        "id": "9LGcDoWZXetw"
      },
      "id": "9LGcDoWZXetw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ces corrélations proches de 1 indiquent que ces variables sont très similaires en termes d’information. Garder toutes ces caractéristiques pourrait augmenter la complexité du modèle sans fournir d'informations supplémentaires significatives.\n",
        "\n",
        "#### Solution 1: Création d'une Caractéristique Synthétique\n",
        "\n",
        "Pour réduire la redondance, nous pouvons combiner ces variables en une caractéristique synthétique plus représentative. Par exemple, nous pourrions créer une caractéristique représentant la densité de chambres par ménage :"
      ],
      "metadata": {
        "id": "F_NZdBtuXvZ6"
      },
      "id": "F_NZdBtuXvZ6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Création d'une nouvelle caractéristique 'rooms_per_household'\n",
        "train_df['rooms_per_household'] = train_df['total_rooms'] / train_df['households']\n",
        "train_df['bedrooms_per_household'] = train_df['total_bedrooms'] / train_df['households']"
      ],
      "metadata": {
        "id": "lRusW9TEyby4"
      },
      "id": "lRusW9TEyby4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution 2: PCA\n",
        "\n",
        "Pour une réduction automatique de la dimensionnalité, on peut utiliser une méthode de réduction de dimensionnalité comme l'analyse en composantes principales (PCA), qui transforme les caractéristiques corrélées en un ensemble de nouvelles caractéristiques non corrélées appelées composantes principales."
      ],
      "metadata": {
        "id": "yjPiYAxoXbPK"
      },
      "id": "yjPiYAxoXbPK"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Appliquer PCA pour réduire la dimensionnalité\n",
        "pca = PCA(n_components=1)\n",
        "train_df['pca_total_rooms_bedrooms_households'] = pca.fit_transform(train_df[['total_rooms', 'total_bedrooms', 'households']])"
      ],
      "metadata": {
        "id": "h4SBK5Rl07Tm"
      },
      "id": "h4SBK5Rl07Tm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La nouvelle caractéristique pca_total_rooms_bedrooms_households représente une combinaison optimisée de total_rooms, total_bedrooms et households, capturant la majorité de leur variance sans redondance."
      ],
      "metadata": {
        "id": "2w2F3XQYY1WB"
      },
      "id": "2w2F3XQYY1WB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEzsbpKjY3oe"
      },
      "id": "UEzsbpKjY3oe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}